<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta property="og:type" content="website">
<meta property="og:title" content="才权的博客">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="才权的博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="才权的博客">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title> 才权的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4e94c4c1ded5d320541370545af638ea";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">才权的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/21/《机器学习》笔记-强化学习（16）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/09/21/《机器学习》笔记-强化学习（16）/" itemprop="url">
                  《机器学习》笔记-强化学习（16）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-21T17:17:45+08:00">
                2018-09-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>任务与奖赏</li>
<li>K-摇臂赌博机</li>
<li>有模型学习</li>
<li>免模型学习</li>
<li>值函数近似</li>
<li>模仿学习<h2 id="（一）任务与奖赏"><a href="#（一）任务与奖赏" class="headerlink" title="（一）任务与奖赏"></a>（一）任务与奖赏</h2>以种西瓜为例。种瓜有许多步骤，从一开始的选种，到定期浇水、施肥、除草、杀虫，经过一段时间才能收获西瓜。通常要等到收获后，我们才知道种出的瓜好不好。若将得到好瓜作为辛勤种瓜劳动奖赏，则在种瓜过程中，当我们执行某个操作（例如，施肥）时，并不能立即获得这个最终奖励，甚至难以判断当前操作对最终奖赏的影响，仅能得到一个当前反馈（例如，瓜苗看起来更健壮了）。我们需多次种瓜，在种瓜过程中不断摸索，然后才能总结出比较好的种瓜策略。这个过程抽象出来，就是“强化学习”（Reinforcement Learning）。<br>下图给出了强化学习的一个简单图示，<br><img src="https://upload-images.jianshu.io/upload_images/4905018-adf37bab2fa2f11b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图16.1 强化学习图示"><br>强化学习任务通常用马尔科夫决策过程（Markov Decision Process，简称MDP）来描述：</li>
<li>机器处于环境E中；</li>
<li>状态空间为X，其中，每个状态x∈X是机器感知环境的描述，如在种瓜任务上，这就是当前瓜苗长势的描述；</li>
<li>机器能采取的动作构成动作空间A，如种瓜过程中有浇水、施不同的肥、使用不同农药等多种可供选择的动作；</li>
<li>若某个动作a∈A作用在当前状态x上，则潜在的转移函数P将使得环境从当前状态按某种概率转移到另一个状态，如瓜苗状态为缺水，若选择动作为浇水，则瓜苗长势会发生变化，瓜苗有一定概率恢复健康，也有一定概率无法恢复；</li>
<li>在转移到另一个状态的同时，环境会根据潜在的“奖赏”（reward）函数R反馈给机器一个奖赏。<br>综合起来，强化学习任务对应了四元组E=<x，a，p，r>。下图给出了一个简单的例子，<br><img src="https://upload-images.jianshu.io/upload_images/4905018-8b16071c473a73eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图16.2 给西瓜浇水问题的马尔科夫决策过程"><br>需要注意“机器”与“环境”的界限，例如，在种西瓜任务中，环境是西瓜生长的自然世界；在下棋对弈中，环境是棋盘与对手；在机器人控制中，环境是机器人的躯体与物理世界。总之，在环境中的状态的转移、奖赏的返回是不受机器控制的，机器只能通过选择要执行的动作来影响环境，也只能通过观察转移后的状态和返回的奖赏来感知环境。<br>机器要做的是通过在环境中不断的尝试而学得一个“策略”（policy）π，根据这个策略，在状态x下就能得知要执行的动作a=π(x)，例如看到瓜苗状态是缺水时，能返回动作“浇水”。<br>策略有两种表示方法：</x，a，p，r></li>
<li>一种是将策略表示为函数π：X-&gt;A，确定性策略常用这种表示；</li>
<li>另一种是概率表示π：XxA-&gt;R，随机性策略常用这种表示，π(x, a)为状态x下选择动作a的概率，这里必须有∑π（x，a）=1；</li>
</ul>
<p>策略的优劣取决于长期执行这一策略后得到的累积奖赏，例如某个策略使得瓜苗枯死，它的累积奖赏会很小，另一个策略种出了好瓜，他的累积奖赏会很大。在强化学习任务中，学习的目标是要找到能使长期累积奖赏最大化的策略。<br>大家也许已经感觉到强化学习与监督学习的差别。若将这里的“状态”对应为监督学习中的“示例”、“动作”对应“标记”，则可看出，强化学习中的“策略”实际上就相当于监督学习中的“分类器”（当动作是离散的）或“回归器”（当动作是连续的），模型的形式并无差别。但不同的是，在强化学习中并没有监督学习中的有标记样本（即“示例-标记”对），换言之，没有人直接告诉机器在什么状态下该做什么动作，只有等到最终结果揭晓，才能通过“反思”之前的动作是否正确来进行学习。因此，强化学习在某种意义上可看做具有“延迟标记信息”的监督学习问题。</p>
<h2 id="（二）K-摇臂赌博机"><a href="#（二）K-摇臂赌博机" class="headerlink" title="（二）K-摇臂赌博机"></a>（二）K-摇臂赌博机</h2><p>与一般监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖赏，即仅考虑一步操作。需要注意的是，即便在这样的简化情形下，强化学习仍与监督学习有明显不同，因为机器需要通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应该做哪个动作。<br>欲最大化单步奖赏需考虑两个方面，</p>
<ul>
<li>一是需要知道每个动作带来的奖赏；</li>
<li>二是要执行奖赏最大的动作；</li>
</ul>
<p>若每个动作对应的奖赏是一个确定值，那么尝试一遍所有的动作便能找出奖赏最大的动作。然而，更一般的情形是，一个动作的奖赏来自于一个概率分布，仅通过一次尝试并不能确切的获得平均奖赏。<br>实际上，单步强化学习任务对应了一个理论模型，即“K-摇臂赌博机”（K-armed bandit），如下图所示，<br><img src="https://upload-images.jianshu.io/upload_images/4905018-5c9068620a7256ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图16.3 K-摇臂赌博机图示"><br>K-摇臂赌博机有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。</p>
<ul>
<li>若仅为获知每个摇臂的期望奖赏，则可采用“仅探索”（exploration-only）法：将所有尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计；</li>
<li>若仅为执行奖赏最大的动作，则可采用“仅利用”（exploitation-only）法：按下目前最优的（即到目前为止平均奖赏最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。</li>
</ul>
<p>显然，“仅探索”法能很好的估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；“仅利用”法则相反，它没有很好的估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累计奖赏最大化。<br>事实上，“探索”（即估计摇臂的优劣）和“利用”（即选择当前最优摇臂）是矛盾的，因为尝试次数（即总投币数）有限，加强了一方面则会削弱另一方，这就是强化学习所面临的“探索-利用窘境”（Exploration-Exploitation dilemma）。显然，欲累计奖赏最大，则必须在探索与利用之间达成较好的折中。</p>
<h3 id="ε-贪心"><a href="#ε-贪心" class="headerlink" title="ε -贪心"></a>ε -贪心</h3><p>ε -贪心基于一个概率来对探索和利用进行折中。</p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>Softmax算法基于当前已知的摇臂平均奖赏来对探索和利用进行折中。</p>
<h2 id="（三）有模型学习"><a href="#（三）有模型学习" class="headerlink" title="（三）有模型学习"></a>（三）有模型学习</h2><p>考虑多步强化学习任务，暂且先假定任务对应的马尔科夫决策过程四元组E=<x，a，p，r>均为已知，这种情形称为“模型已知”，即机器已对环境进行了建模，能在机器内部模拟出与环境相同或相近的情况。在已知模型的环境中学习称为“有模型学习”（model-based Learning）。</x，a，p，r></p>
<h2 id="（四）免模型学习"><a href="#（四）免模型学习" class="headerlink" title="（四）免模型学习"></a>（四）免模型学习</h2><p>在现实的强化学习任务中，环境的转移概率、奖励函数往往很难得知，甚至很难知道环境中一共有多少状态。若学习算法不依赖于环境建模，则称为“免模型学习”（model-free learning），这比有模型学习要困难得多。</p>
<h2 id="（五）值函数近似"><a href="#（五）值函数近似" class="headerlink" title="（五）值函数近似"></a>（五）值函数近似</h2><p>前面我们一直假定强化学习任务是在有限状态空间上进行，每个状态可用一个编号来指代。然而，现实强化学习任务所面临的状态空间往往是连续的，有无穷多个状态。这该怎么办呢？<br>一个直接的想法是对状态空间进行离散化，将连续状态空间转化为有限离散状态空间，然后就能使用前面介绍的方法求解。遗憾的是，如何有效的对状态空间进行离散化是一个难题，尤其是在对状态空间进行搜索之前。<br>实际上，我们不妨直接对连续状态空间的值函数进行学习。</p>
<h2 id="（六）模仿学习"><a href="#（六）模仿学习" class="headerlink" title="（六）模仿学习"></a>（六）模仿学习</h2><p>在强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后的累积奖赏，但现实任务中，往往能得到人类专家的决策过程范例。例如在种瓜任务上得到农业专家的种植过程范例。从这样的范例中学习，称为“模仿学习”（imitation learning）。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/17/《机器学习》笔记-规则学习（15）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/17/《机器学习》笔记-规则学习（15）/" itemprop="url">
                  《机器学习》笔记-规则学习（15）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-17T23:56:54+08:00">
                2018-05-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>基本概念</li>
<li>序贯覆盖</li>
<li>剪枝优化</li>
<li>一阶规则学习</li>
<li>归纳逻辑程序设计</li>
</ul>
<h2 id="（一）基本概念"><a href="#（一）基本概念" class="headerlink" title="（一）基本概念"></a>（一）基本概念</h2><p>规则学习是“符号主义学习”（symbolism learning）的主要代表，是最早开始研究的机器学习技术之一。<br>机器学习中的“规则”（rule）通常是指语义明确、能描述数据分布所隐含的客观规律或领域概念、可写成“若…，则…”形式的逻辑规则。“规则学习”（rule learning）是从训练数据中学习出一组能用于对未见示例进行判别的规则。一般形式如下所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-da75da3c5bba2d00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="15.1"><br>与神经网络、支持向量机这样的“黑箱模型”相比，规则学习具有更好的可解释性，能使用户更直观地对判别过程有所了解。另一方面，数理逻辑具有极强的表达能力，绝大多数人类知识都能通过数理逻辑进行简洁的刻画和表达。因此，规则学习能更自然的在学习过程中引入领域知识。此外，逻辑规则的抽象描述能力在处理一些高复杂度的AI任务时具有显著的优势，例如在问答系统中有时可能遇到非常多、甚至无穷种可能的答案，此时若能基于逻辑规则进行抽象表述或者推理，则将带来极大的便利。<br>从形式语言表达能力而言，规则可分为两类：</p>
<ul>
<li>”命题规则“（propositional rule）<br>由”原子命题“（propositional atom）和逻辑连接词“与”、“或”、“非”和“蕴含”构成的简单陈述句；</li>
<li>“一阶规则”（first-order rule）<br>基本成分是能描述事物的属性或关系的“原子公式”（automic formula）；</li>
</ul>
<p>从形式语言系统的角度来看，命题规则是一阶规则的特例，因此一阶规则的学习比命题规则要复杂得多。</p>
<h2 id="（二）序贯覆盖"><a href="#（二）序贯覆盖" class="headerlink" title="（二）序贯覆盖"></a>（二）序贯覆盖</h2><p>规则学习的目标是产生一个能覆盖尽可能多的样例规则集。最直接的做法是“序贯覆盖”（sequential covering），即逐条归纳：在训练集上每学到一条规则，就将该规则覆盖的训练样例去除，然后以剩下的训练样例组成训练集重复上述过程。由于每次只处理一部分数据，因此也被称为“分治”（separate-and-conquer）策略。<br>由于序贯覆盖法简单有效，几乎所有规则学习算法都以它为基本框架。它能方便地推广到多分类问题上，只需将每类分别处理即可。</p>
<h2 id="（三）剪枝优化"><a href="#（三）剪枝优化" class="headerlink" title="（三）剪枝优化"></a>（三）剪枝优化</h2><p>规则生成本质是一个贪心搜索过程，需有一定的机制来缓解过拟合的风险，最常见的做法是剪枝（pruning）。与决策树相似，剪枝可发生在规则生长过程中，即“预剪枝”，也可发生在规则产生后，即“后剪枝”。通常是基于某种性能度量指标来评估增/删逻辑文字前后的规则性能，或增/删规则前后的规则集性能，从而判断是否要进行剪枝。</p>
<h2 id="（四）一阶规则学习"><a href="#（四）一阶规则学习" class="headerlink" title="（四）一阶规则学习"></a>（四）一阶规则学习</h2><p>受限于命题规则表达能力，命题规则学习难以处理对象之间的“关系”（relation），而关系信息在很多任务中非常重要。需用一阶逻辑表示，并且要使用一阶规则学习。</p>
<p>##（五）归纳逻辑程序设计<br>归纳逻辑程序设计（Inductive Logic Programming，简称ILP）在一阶规则学习中引入了函数和逻辑表达式嵌套。一方面，这使得机器学习系统具备了更为强大的表达能力；另一方面，ILP可看作用机器学习技术来解决基于背景知识的逻辑程序（logic program）归纳，其学得的“规则”可被PROLOG等逻辑程序设计语言直接使用。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/13/《机器学习》笔记-概率图模型（14）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/13/《机器学习》笔记-概率图模型（14）/" itemprop="url">
                  《机器学习》笔记-概率图模型（14）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-13T22:26:23+08:00">
                2018-05-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>隐马尔可夫模型</li>
<li>马尔可夫随机场</li>
<li>条件随机场</li>
<li>学习与推断</li>
<li>近似推断</li>
<li>话题模型</li>
</ul>
<h2 id="（一）隐马可科夫模型"><a href="#（一）隐马可科夫模型" class="headerlink" title="（一）隐马可科夫模型"></a>（一）隐马可科夫模型</h2><p>机器学习最重要的任务，是根据一些已观察到的证据（例如训练样本）来对感兴趣的未知变量（例如类别标记）进行估计和推测。概率模型（probabilistic model）提供了一种描述框架，将学习任务归结于计算变量的概率分布。在概率模型中，利用已知变量推测位置变量的分布称为“推断”（inference），其核心是如何基于可观测变量推测出未知变量的条件分布。具体来说，假定所关心的变量集合为Y，可观测变量集合为O，其他变量集合为R，</p>
<ul>
<li>“生成式”（generative）模型考虑联合分布P(Y,R,O)；</li>
<li>“判别式”（discriminative）模型考虑条件分布P(Y,R|O);</li>
</ul>
<p>给定一组观测变量值，推断就是由P(Y,R,O)或P(Y,R|O)得到条件分布P(Y|O)。<br>直接利用概率和规则消去变量R显然不可行。为了便于研究高效的推断和学习算法，需要有一套能简洁紧凑地表达变量间关系的工具。<br>概率图模型（probabilistic graphical model）是一类用图来表达变量相关关系的概率模型。它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，结点之间的边表示变量间的概率相关关系，即“变量关系图”。根据边的性质不同，概率图模型可大致分为两类：</p>
<ul>
<li>第一类是使用有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网（Bayesian network）；</li>
<li>第二类是使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网（Markov network）；</li>
</ul>
<p>隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的动态贝叶斯网（dynamic Bayesian network），这是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用。<br>隐马尔可夫模型中的变量可分为两组。第一组是状态变量{y1，y2，…，yn}，其中，yi∈Y表示第i时刻的系统状态。通常假定状态变量是隐藏的、不可被观测的，因此状态变量亦称隐变量（hidden variable）。第二组是观测变量{x1，x2，…，xn}，其中，xi∈X表示第i时刻的观测值。在隐马尔可夫模型中，系统通常在多个状态{s1，s2，…，sN}之间转换。如下图所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-819f2b96464d66fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图14.1"><br>在任一时刻，观测变量的取值仅依赖于状态变量，即xt由yt确定，与其他状态变量及观测变量的取值无关。同时，t时刻的状态yt仅依赖于<br>t-1时刻的状态yt-1，与其余n-2个状态无关。这就是所谓的“马尔可夫链”（Markov chain），即：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态。<br>在实际应用中，人们常常关注隐马尔可夫模型的三个基本问题：</p>
<ul>
<li>如何评价模型与观察序列之间的匹配程度<br>例如许多任务需根据以往的观察序列{x1，x2，…，xn-1}来推测当前时刻最可能的观测值xn；</li>
<li>如何根据观测序列推断出隐藏的模型状态<br>例如在语音识别等任务中，观测值为语音信号，隐藏状态为文字，目标就是根据观测信号来推断最有可能的状态序列（即对应的文字）；</li>
<li>如何训练模型使其能最好的描述观测数据<br>例如在大多数现实应用中，人工指定模型参数已变得越来越不可行，如何根据训练样本学得最优的模型参数；</li>
</ul>
<h2 id="（二）马尔可夫随机场"><a href="#（二）马尔可夫随机场" class="headerlink" title="（二）马尔可夫随机场"></a>（二）马尔可夫随机场</h2><p>马尔可夫随机场（markov Random Field，简称MRF）是典型的马尔可夫网，这是一种著名的无向图模型。图中每个结点表示一个或一组变量，结点之间的边表示两个变量之间的依赖关系。马尔可夫随机场有一组势函数（potential function），亦称“因子”（factor），这是定义在变量子集上的非负函数，主要用于定义概率分布模型。</p>
<h2 id="（三）条件随机场"><a href="#（三）条件随机场" class="headerlink" title="（三）条件随机场"></a>（三）条件随机场</h2><p>条件随机场（Conditional Random Field，简称CRF）是一种判别式无向图模型。生成式模型是直接对联合分布进行建模，而判别式模型则是对条件分布进行建模。前面介绍的隐马尔可夫模型和马尔可夫随机场都是生成式模型，而条件随机场是判别式模型。</p>
<h2 id="（四）学习与推断"><a href="#（四）学习与推断" class="headerlink" title="（四）学习与推断"></a>（四）学习与推断</h2><p>基于概率图模型定义的联合概率分布，我们能对目标变量的边际分布（marginal distribution）或以某些可观测变量为条件的条件分布进行推断。<br>对概率图模型，还需确定具体分布的参数，这称为参数估计或参数学习问题。<br>概率图模型的推断方法大致可分为两类：</p>
<ul>
<li>第一类是精确推断方法<br>希望能计算出目标变量的边际分布或条件分布的精确值。遗憾的是，一般情形下，此类算法的计算复杂度随着极大团规模的增长呈指数增长，适用范围有限。</li>
<li>第二类是近似推断方法<br>希望在较低时间复杂度下获得原问题的近似解。此类方法在现实任务中更常用。</li>
</ul>
<p>精确推断具有代表性的方法有，</p>
<h3 id="1-变量消去"><a href="#1-变量消去" class="headerlink" title="1.变量消去"></a>1.变量消去</h3><p>精确推断的实质是一类动态规划算法，它利用图模型所描述的条件独立性来消减计算目标概率值所需的计算量。变量消去是最直观的精确推断算法，也是构建其他精确推断算法的基础。<br>变量消去法有一个明显的缺陷：若需计算多个边际分布，重复使用变量消去法将对造成大量的冗余计算。</p>
<h3 id="2-信念传播"><a href="#2-信念传播" class="headerlink" title="2. 信念传播"></a>2. 信念传播</h3><p>信念传播（Belief Propagation）算法将变量消去法中的求和操作看作一个消息传递过程，较好的解决了求解多个边际分布时重复计算问题。</p>
<h2 id="（五）近似推断"><a href="#（五）近似推断" class="headerlink" title="（五）近似推断"></a>（五）近似推断</h2><p>精确推断方法通常需要很大的计算开销，因此在现实应用中近似推断方法更为常用。近似推断方法大致可分为两大类：</p>
<ul>
<li>第一类是采样（sampling）<br>通过使用随机化方法完成近似；</li>
<li>第二类是使用确定性近似完成近似推断<br>典型代表为变分推断（variational inference）；</li>
</ul>
<h3 id="1-MCMC采样"><a href="#1-MCMC采样" class="headerlink" title="1. MCMC采样"></a>1. MCMC采样</h3><p>概率图模型中最常用的采用技术是马尔可夫链蒙特卡罗（Markov Chain Monte Carlo，简称MCMC）方法。</p>
<h3 id="2-变分推断"><a href="#2-变分推断" class="headerlink" title="2. 变分推断"></a>2. 变分推断</h3><p>变分推断通过使用已知简单分布来逼近所需推断的复杂分布，并通过限制近似分布的类型，从而得到一种局部最优、但具有确定解的近似后验分布。</p>
<h2 id="（六）话题模型"><a href="#（六）话题模型" class="headerlink" title="（六）话题模型"></a>（六）话题模型</h2><p>话题模型（topic model）是一族生成式有向图模型，主要用于处理离散型的数据（如文本集合），在信息检索、自然语言处理等领域有广泛应用。隐狄利克雷分配模型（Latent Dirichlet Allocation，简称LDA）是话题模型的典型代表。<br>话题模型中有几个重要概念：词（word）、文档（document）和话题（topic）。</p>
<ul>
<li>词<br>“词”是待处理数据的基本离散单元，例如在文本处理任务中，一个词就是一个英文单词或有独立意义的中文词。</li>
<li>文档<br>“文档”是待处理的数据对象，它由一组词组成，这次词在文档中是不计顺序的，例如一篇论文、一个网页都可看做一个文档；这种表示方式称为“词袋”（bag-of-words）。数据对象只要能用词袋描述，就可使用话题模型。</li>
<li>话题<br>“话题”表示一个概念，具体表示为一系列相关的词，以及它们在该概念下出现的概率。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/02/《机器学习》笔记-半监督学习（13）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/02/《机器学习》笔记-半监督学习（13）/" itemprop="url">
                  《机器学习》笔记-半监督学习（13）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-02T23:15:09+08:00">
                2018-05-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>未标记样本</li>
<li>生成式方法</li>
<li>半监督SVM</li>
<li>图半监督学习</li>
<li>基于分歧的方法</li>
<li>半监督聚类</li>
</ul>
<h2 id="（一）未标记样本"><a href="#（一）未标记样本" class="headerlink" title="（一）未标记样本"></a>（一）未标记样本</h2><p>让学习器不依赖外界交互，自动地利用未标记样本来提升学习性能，就是半监督学习(semi-supervised learning)。<br>要利用未标记样本，必然要做一些未标记样本所揭示的数据分布信息与类别标记相联系的假设。最常见的是“聚类假设”（cluster assumption），即假设数据存在簇结构，同一个簇样本属于同一个类别。半监督学习中另一个常见假设是“流形假设”（manifold assumption），即假设数据分布在一个流形结构上，邻近的样本拥有相似的输出值。“邻近”程度常用“相似”程度来刻画，因此，流行假设可看作聚类假设的推广，但流形假设对输出值没有限制，因此比聚类假设的适用范围更广，可用于更多类型的学习任务。事实上，无论聚类假设还是流形假设，其本质都是“相似的样本拥有相似的输出”这个基本假设。<br>半监督学习可进一步划分为纯（pure）半监督学习和直推学习（transductive learning），前者假定训练数据中的未标记样本并非待预测数据，而后者则假定学习过程中所考虑的未标记样本恰是待预测数据，学习的目的就是在这些未标记样本上获得最优泛化性能。换言之，纯半监督学习是基于“开放世界”假设，希望学得模型能适用于训练过程中未观察到的数据；而直推学习是基于“封闭世界”假设，仅试图对学习过程中观察到的未标记数据进行预测。如下图所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-6ff5175835dedf29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图13.2"></p>
<h2 id="（二）生成式方法"><a href="#（二）生成式方法" class="headerlink" title="（二）生成式方法"></a>（二）生成式方法</h2><p>生成式方法（generative methods）是直接基于生成式模型的方法。此类方法假设所有数据（无论是否有标记）都是由同一个潜在的模型“生成”的。这个假设使得我们能通过潜在模型的参数将未标记数据与学习目标联系起来，而未标记数据的标记则可看作模型的缺失参数，通常可基于EM算法进行极大似然估计求解。此类方法的区别主要在于生成式模型的假设，不同的模型假设将产生不同的方法。</p>
<h2 id="（三）半监督SVM"><a href="#（三）半监督SVM" class="headerlink" title="（三）半监督SVM"></a>（三）半监督SVM</h2><p>半监督支持向量机（Semi-Supervised Support Vector Machine，简称 S3VM）是支持向量机在半监督学习上的推广。在不考虑未标记样本时，支持向量机试图找到最大间隔划分超平面，而在考虑未标记样本后，S3VM试图找到能将两类有标记样本分开，且穿过数据低密度区域的划分超平面。如下图所示，<br><img src="https://upload-images.jianshu.io/upload_images/4905018-326ab3b967720f7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图13.3"></p>
<p>这里的基本假设是“低密度分隔”（low-density separation），显然，这是聚类假设在考虑了线性超平面划分后的推广。</p>
<h2 id="（四）图半监督学习"><a href="#（四）图半监督学习" class="headerlink" title="（四）图半监督学习"></a>（四）图半监督学习</h2><p>给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图中一个结点，若两个样本之间的相似度很高（或相关性很强），则对应结点之间存在一条边，边的“强度”（strength）正比于样本之间的相似度（或相关性）。我们可将有标记样本所对应的结点想象为染过色，而未标记样本所对应的结点尚未染色。于是，半监督学就对应于“颜色”在图上扩散或传播的过程。由于一个图对应了一个矩阵，这使得我们能基于矩阵运算来进行半监督学习算法的推到和分析。<br>图半监督学习方法在概念上相当清晰，且易于通过对所涉矩阵运算的分析来探索算法性质。但此类算法的缺陷也相当明显。首先是在存储开销上，若样本数为O(m)，则算法中所涉及的矩阵规模未O(m2)，这使得此类算法很难直接处理大规模数据；另一方面，由于构图过程仅能考虑训练样本集，难以判断新样本在图中的位置，因此，在接收到新样本时，或是将其加入原数据集对图进行重构并重新进行标记传播，或是需引入额外的预测机制。</p>
<h2 id="（五）基于分歧的方法"><a href="#（五）基于分歧的方法" class="headerlink" title="（五）基于分歧的方法"></a>（五）基于分歧的方法</h2><p>与生成式方法、半监督SVM、图半监督学习等基于单学习器利用未标记数据不同，基于分歧的方法（disagreement-base methods）使用多学习器，而学习器之间的“分歧”（disagreement）对未标记数据的利用至关重要。<br>基于分歧的方法只需采用合适的基学习器，就能较少受到模型假设、损失函数非凸性和数据规模的影响，学习方法简单有效、理论基础相对坚实、适用范围较为广泛。为了使用此类方法，需能生成具有显著分歧、性能尚可的多个学习器，但当有标记样本很少，尤其是数据不具有多视图时，要做到这一点并不容易，需有技巧的设计。</p>
<h2 id="（六）半监督聚类"><a href="#（六）半监督聚类" class="headerlink" title="（六）半监督聚类"></a>（六）半监督聚类</h2><p>聚类是一种典型的无监督学习任务，然而在现实聚类任务中我们往往能获得一些额外的监督信息，于是可通过半监督聚类（semi-supervised clustering）来利用监督信息以获得更好的聚类效果。<br>聚类任务中获得的监督信息大致有两种类型。第一种类型是“必连”（must-link）与“勿连”（cannot-link）约束，前者是指样本必属于同一个簇，后者是指样本必不属于同一个簇；第二种类型的监督信息则是少量的有标记样本。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/15/《机器学习》笔记-计算学习理论（12）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/15/《机器学习》笔记-计算学习理论（12）/" itemprop="url">
                  《机器学习》笔记-计算学习理论（12）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-15T23:50:26+08:00">
                2018-04-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>基础知识</li>
<li>PAC学习</li>
<li>有限假设空间</li>
<li>VC维</li>
<li>Rademacher复杂度</li>
<li>稳定性</li>
</ul>
<h2 id="（一）基础知识"><a href="#（一）基础知识" class="headerlink" title="（一）基础知识"></a>（一）基础知识</h2><p>顾名思义，计算学习理论(computation learning theory)研究的是关于通过“计算”来进行“学习”的理论。即关于机器学习的理论基础，其目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。</p>
<h2 id="（二）PAC学习"><a href="#（二）PAC学习" class="headerlink" title="（二）PAC学习"></a>（二）PAC学习</h2><p>计算学习理论中最基本的是概率近似正确(Probably Approximately Correct，简称PAC)学习理论。<br>给定训练集D，我们希望基于学习算法ξ学得的模型所对应的假设h尽可能接近目标概念c。<br>PAC学习中一个关键因素是假设空间H的复杂度。H包含了学习算法ξ所有可能输出的假设，若在PAC学习中假设空间与概念类完全相同，即H=C，这称为“恰PAC可学习”（properly PAC learnable）；直观地看，这意味着学习的能力与学习任务“恰好匹配”。然而，这种让所有候选假设都来自概念类的要求看似合理，但却并不实际，因为在现实应用中我们对概念类C通常一无所知，更别说获得一个假设空间与概念类恰好相同的学习算法。显然，更重要的研究假设空间与概念类不同的情形，即H≠C。一般而言，H越大，其包含任意目标概念的可能性越大，但从中找到某个具体目标概念的难道也越大。|H|有限时，我们称H为“有限假设空间”，否则称为“无限假设空间”。</p>
<h2 id="（三）有限假设空间"><a href="#（三）有限假设空间" class="headerlink" title="（三）有限假设空间"></a>（三）有限假设空间</h2><ul>
<li>可分情形<br>可分情形意味着目标概念c属于假设空间H，即c∈H；</li>
<li>不可分情形<br>对较为困难的学习问题，目标概念c往往不存在与假设空间H中；</li>
</ul>
<h2 id="（四）VC维"><a href="#（四）VC维" class="headerlink" title="（四）VC维"></a>（四）VC维</h2><p>现实学习任务所面临的通常是无限假设空间。欲对此种情形的学习性进行研究，需度量假设空间的复杂度。最常见的办法是考虑假设空间的“VC维”（Vapnik-Chervonenkis dimension）。</p>
<h2 id="（五）Rademacher复杂度"><a href="#（五）Rademacher复杂度" class="headerlink" title="（五）Rademacher复杂度"></a>（五）Rademacher复杂度</h2><p>基于VC维的泛化误差界是分布无关、数据独立的，也就是说，对任何数据分布都成立。这使得基于VC维的学习性分析结果具有一定的“普适性”；但从另一个方面来说，由于没考虑数据自身，基于VC维得到的泛化误差界通常比较“松”，对那些与学习问题的典型情况相差甚远的较“坏”分布来说尤其如此。<br>Rademacher复杂度（Rademacher complexity）是另一种刻画假设空间复杂度的途径，与VC维不同的是，它在一定程度上考虑了数据分布。</p>
<h2 id="（六）稳定性"><a href="#（六）稳定性" class="headerlink" title="（六）稳定性"></a>（六）稳定性</h2><p>无论是基于VC维还是Rademacher复杂度来推导泛化误差界，所得到的结果均与具体学习算法无关。对所有学习算法都适用。这使得人们能够脱离具体学习算法的设计来考虑学习问题本身的性质。但在另一方面，若希望获得与算法有关的分析结果，则需另辟蹊径。稳定性（stability）分析是这方面一个值得关注的方向。<br>顾名思义，算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/23/《机器学习》笔记-特征选择与稀疏学习（11）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/23/《机器学习》笔记-特征选择与稀疏学习（11）/" itemprop="url">
                  《机器学习》笔记-特征选择与稀疏学习（11）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-23T11:52:31+08:00">
                2018-02-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>子集搜索与评价</li>
<li>过滤式选择</li>
<li>包裹式选择</li>
<li>嵌入式选择与L1正则化</li>
<li>稀疏表示与字典学习</li>
<li>压缩感知</li>
</ul>
<h2 id="（一）子集搜索与评价"><a href="#（一）子集搜索与评价" class="headerlink" title="（一）子集搜索与评价"></a>（一）子集搜索与评价</h2><p>我们称样本属性为“特征”（feature），对当前学习任务有用的属性称为“相关特征”（relevant feature）、没什么用的属性称为“无关特征”（inrelevant feature）。从特征集合中选择相关特征子集的过程，称为“特征选择”（feature selection）。<br>特征选择是一个重要的“数据预处理”（data preprocessing）过程，在现实机器学习中，获得数据之后通常进行特征选择，之后再进行训练学习器。<br>特征选择的原因主要包括：</p>
<ul>
<li>首先，我们在现实任务中经常会遇到维数灾难问题，这是由于属性过多而造成的，若能从中选择出重要的特征，使得后续学习过程仅需要在一部分特征上构建模型，则维数灾难问题会大为减轻（降维和特征选择是处理高维数据的两大主流技术）。</li>
<li>其次，去除不相关特征，只留下关键因素，往往会降低学习任务的难度。</li>
</ul>
<p>从初始的特征集合中选取一个包含了所有重要信息的特征子集，涉及两个关键环节：</p>
<ul>
<li>“子集搜索”（subset search）<br>搜索策略包括“前向”（forward）搜索，“后向”（backward）搜索和”双向“（bidirectional）搜索；</li>
<li>”子集评价“（subset evaluation）<br>子集评价采用信息增益作为评价准则；</li>
</ul>
<p>将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。<br>常见的特征选择方法大致可分为三类：过滤式（filter）、包裹式（wrapper）和嵌入式（embedding）。</p>
<h2 id="（二）过滤式选择"><a href="#（二）过滤式选择" class="headerlink" title="（二）过滤式选择"></a>（二）过滤式选择</h2><p>过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行”过滤“，再用过滤后的特征来训练模型。<br>其中，Relief（Relevant Feature）是一种著名的过滤式特征选择方法，该方法设计了一个”相关统计量“来度量特征的重要性。</p>
<h2 id="（三）包裹式选择"><a href="#（三）包裹式选择" class="headerlink" title="（三）包裹式选择"></a>（三）包裹式选择</h2><p>与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则。换言之，包裹式特征选择的目的就是为给定的学习器选择最有利其性能、“量身定做”的特征子集。<br>一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好，但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大得多。<br>LVW(Las Vegs Wrapper)式一个典型的包裹式特征选择方法。</p>
<h2 id="（四）嵌入式选择与L1正则化"><a href="#（四）嵌入式选择与L1正则化" class="headerlink" title="（四）嵌入式选择与L1正则化"></a>（四）嵌入式选择与L1正则化</h2><p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明确的分别；与此不同，嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一过程中完成，即在学习器训练过程中自动地进行了特征选择。<br>当样本特征很多，而样本数较少时，训练很容易陷入过拟合。为了缓解过拟合问题，引入正则化项。<br>L1范数和L2范数都有助于降低过拟合风险，但前者会带来一个额外的好处：它比后者更容易获得“稀疏(sparse)解”，即它求得的w会有更少的非零分量。</p>
<h2 id="（五）稀疏表示与字典学习"><a href="#（五）稀疏表示与字典学习" class="headerlink" title="（五）稀疏表示与字典学习"></a>（五）稀疏表示与字典学习</h2><p>把数据集D考虑成一个矩阵，其每一行对应于一个样本，每列对应与于一个特征。特征选择所考虑的问题是特征具有“稀疏性”，即矩阵中的许多列与当前学习任务无关，通过特征寻找去除这些列，则学习器训练过程仅需在较小的矩阵上进行，学习任务的难度可能有所降低，涉及的计算和存储开销会减少，学得模型的可解释性也会提高。<br>现在考虑另一种稀疏性：D所对应的矩阵中存在很多零元素，但这些零元素并不是以整列、整行形式存在的。<br>当样本具有这样的稀疏表达形式时，对学习任务来说会有不少好处。例如具有高度的稀疏性使大多数问题变得线性可分。同时，稀疏样本并不会造成存储上的巨大负担，因为稀疏矩阵已有很多高效的存储方法。<br>为普通稠密表达的样本找到合适的字典，将样本转化为合适的稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为”字典学习“（dictionary learning），亦称”稀疏编码“(sparse coding)。这两个称谓稍有差别，”字典学习“更侧重于学得字典的过程，而”稀疏编码“则更侧重于对样本稀疏表达的过程。</p>
<h2 id="（六）压缩感知"><a href="#（六）压缩感知" class="headerlink" title="（六）压缩感知"></a>（六）压缩感知</h2><p>与特征选择、稀疏表示不同，压缩感知关注的是如何利用信号本身所具备的稀疏性，从部分观测样本中恢复原信号。通常认为，压缩感知分为”感知测量“和”重构恢复“这两个阶段。”感知测量“关注如何对原始信号进行处理以获得稀疏样本表示；”重构恢复“关注的是如何基于稀疏性从少量观测中恢复原信号，这是压缩感知的精髓，当我们谈到压缩感知时，通常是指该部分。<br>基于部分信息来恢复全部信息的技术在许多现实任务中有重要应用。例如网上书店通过收集读者在网上对书的评价，可根据读者的读书偏好来进行新书推荐，从而达到定向广告投放的效果。显然，没有哪位读者读过所有的书，也没有那本书被所有读者读过，因此，网上书店所搜索到的仅有部分信息，如下图所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-20da5f23d9fdba8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="表11.1"></p>
<p>那么，能够将上图中通过读者评价得到的数据当做部分信号，基于压缩感知的思想恢复出完整的信号呢？<br>矩阵补全（matrix completion）技术可用于解决这个问题。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/10/《机器学习》笔记-降维与度量学习（10）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/10/《机器学习》笔记-降维与度量学习（10）/" itemprop="url">
                  《机器学习》笔记-降维与度量学习（10）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-10T10:47:15+08:00">
                2018-02-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>k近邻学习</li>
<li>低纬嵌入</li>
<li>主成分分析</li>
<li>核化线性降维</li>
<li>流形学习</li>
<li>度量学习</li>
</ul>
<h2 id="（一）k近邻学习"><a href="#（一）k近邻学习" class="headerlink" title="（一）k近邻学习"></a>（一）k近邻学习</h2><p>k近邻（k-Nearest，简称kNN）学习是一种常用的监督学习方法，其工作机制非常简单：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本。然后基于这k个“邻居”的信息进行预测。通常，在分类任务中可使用“投票法”，即选择这k个样本中出现最多的类别标记作为预测结果；在回归任务中可使用“平均法”，即将这k个样本的实际值输出标记的平均值作为预测结果。还可基于距离远近的加权平均或加权投票，距离越近的样本权重越大。<br>与先前介绍的学习方法相比，k近邻学习有一个明显的不同之处，它似乎没有显示的训练过程。事实上，它是“懒惰学习”（lazy learning）的著名代表，此类学习技术在训练阶段仅仅是将样本存储起来，训练开销为零，待收到测试样本后再进行处理；相应的，那些在训练阶段就对样本进行学习处理的方法，称为“急切学习”（eager learning）。<br>下图给出了k近邻分类器的一个示意图，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-e32c732fa967b465.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图10.1"><br>近邻分类器虽简单，但它的泛化错误率不超过贝叶斯最优分类器的错误率的两倍。</p>
<h2 id="（二）低维嵌入"><a href="#（二）低维嵌入" class="headerlink" title="（二）低维嵌入"></a>（二）低维嵌入</h2><p>上一节的讨论是基于一个重要假设：任意测试样本x附近任意小的δ距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为“密采样”（dense sample）。然而，这个假设在现实任务中通常很难满足。此外，许多学习方法都涉及距离计算，而高维空间会给距离计算带来很大麻烦，例如当维数很高时，甚至连计算内积都不再容易。<br>事实上，在高维情形下出现的数据样本稀疏，计算距离困难等问题，是所有机器学习方法共同面临的严重障碍，被称为“维数灾难”（curse of dimensionality）。<br>缓解维数灾难的一个重要途径是降维（dimension reduction），亦称“维数简约”，即通过某种数学变换将原始高纬度属性空间转变为一个低维“子空间”（subspace），在这个子空间中样本密度大幅提高，计算距离也变得更为容易。为什么能进行降维？这是因为在很多时候，人们人们观测或收集到的数据样本虽是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间的一个底维“嵌入”（embedding）。下图给出了一个直观的例子，原始高维空间中的样本点，在这个低纬嵌入子空间中更容易进行学习。<br><img src="http://upload-images.jianshu.io/upload_images/4905018-9301d899e9040095.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图10.2"><br>若要求原始空间中的样本之间距离在低维空间中得以保持，如上图所示，即得到“多维缩放”（Multiple Dimensional Scaling，简称MDS）这样一种经典的降维方法。<br>对降维效果的评估，通常是比较降维前后学习器的性能，若性能有所提高则认为降维起到了作用。若维数降低到二维或三维，则可通过可视化技术来直观的判断降维效果。</p>
<h2 id="（三）主成分分析"><a href="#（三）主成分分析" class="headerlink" title="（三）主成分分析"></a>（三）主成分分析</h2><p>主成分分析（Principal Component Analysis，简称PCA）是最常用的一种降维方法。<br>PCA仅需保留W与与样本的均值向量即可通过简单的向量减法和矩阵向量乘法将样本投影到低维空间中。显然，低维空间与原始高维空间必有不同，因为对应于最小的d-d’个特征值的特征向量被舍弃了，这是降维导致的结果。但舍弃这样的信息往往是必要的：</p>
<ul>
<li>一方面，舍弃这部分信息之后能使样本的采样密度增大，这正是降维的重要动机；</li>
<li>另一方面，当数据受到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到去噪的效果。</li>
</ul>
<h2 id="（四）核化线性降维"><a href="#（四）核化线性降维" class="headerlink" title="（四）核化线性降维"></a>（四）核化线性降维</h2><p>线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少实际任务中，可能需要非线性映射才能找到恰当的低维嵌入。<br>非线性降维的一种常用方法，是基于核技巧对线性降维方法进行”核化“（kernelized）。</p>
<h2 id="（五）流形学习"><a href="#（五）流形学习" class="headerlink" title="（五）流形学习"></a>（五）流形学习</h2><p>流形学习（manifold learning）是一类借鉴了拓扑流形概念的降维方法。”流形“是在局部与欧氏空间同胚的空间，换言之，它在局部具有欧氏空间的性质，能用欧氏距离来进行距离计算。这给降维方法带来了很大的启发。若低维流形嵌入到高维空间中，则数据样本在高维空间的分布虽然看上去非常复杂，但在局部上仍具有欧氏空间的性质，因此，可以容易的在局部建立降维映射关系，然后，再设法将局部映射关系推广到全局。当维数被降至二维或三维时，能对数据进行可视化展示，因此流形学习也可被用于可视化。<br>其中，等量度映射和局部线性嵌入式两种著名的流形学习方法。</p>
<h2 id="（六）度量学习"><a href="#（六）度量学习" class="headerlink" title="（六）度量学习"></a>（六）度量学习</h2><p>在机器学习中，对高维数据进行降维的主要目的是希望找到一个合适的低维空间，在此空间中进行学习能比原始空间性能更好。事实上，每个空间对应了在样本属性上定义的一个距离度量，而寻找合适的空间，实质上就是在寻找一个合适的距离度量。直接尝试”学习“出一个合适的距离度量，就是度量学习（metric learning）的基本动机。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/08/《机器学习》笔记-聚类（9）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/08/《机器学习》笔记-聚类（9）/" itemprop="url">
                  《机器学习》笔记-聚类（9）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-08T09:00:32+08:00">
                2018-02-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>聚类任务</li>
<li>性能度量</li>
<li>距离计算</li>
<li>原型聚类</li>
<li>密度聚类</li>
<li>层次聚类</li>
</ul>
<h2 id="（一）聚类任务"><a href="#（一）聚类任务" class="headerlink" title="（一）聚类任务"></a>（一）聚类任务</h2><p>在无监督学习中（unsupervised learning）中，训练样本的标记信息是未知的，目标是通过对无标记的训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。此类学习任务中研究最多、应用最广的是“聚类”（clustering）。<br>聚类试图将数据集中的样本划分为若干通常是不相交的子集，每个子集称为一个“簇”（cluster）。<br>聚类既能作为一个单独的过程，用于找寻数据内的分布结构，也可作为分类等其他学习任务的前驱过程。</p>
<h2 id="（二）性能度量"><a href="#（二）性能度量" class="headerlink" title="（二）性能度量"></a>（二）性能度量</h2><p>聚类性能度量亦称聚类“有效性指标”（validity index）。与监督学习中的性能度量作用相似。对聚类结果，我们需通过某种性能度量来评估其好坏；另一方面，若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得到符合要求的聚类结果。<br>聚类是将样本集D划分为若干不相交的子集，即样本簇。直观上看，我们希望“物以类聚”，即同一簇的样本尽可能彼此相似，不同簇的样本尽可能不同。换言之，聚类结果的“簇内相似度”（intra-cluster similarity）高且“簇间相似度”（inter-cluster similarity）低。<br>聚类性能度量大致有两类：</p>
<ul>
<li>“外部指标”（external index）<br>将聚类结果与某个“参考模型”（reference model）进行比较；</li>
<li>“内部指标”（internal index）<br>直接考察聚类结果而不利用任何参考模型；</li>
</ul>
<p>常用的聚类性能度量外部指标有：</p>
<ul>
<li>Jaccard系数（Jaccard Coefficient，简称 JC）</li>
<li>FM指数（Fowlkes and Mallows Index，简称FMI）</li>
<li>Rand指数（Rand Index，简称RI）</li>
</ul>
<p>常用的聚类性能度量内部指标有：</p>
<ul>
<li>DB指数（Davies-Bouldin Index，简称DBI）</li>
<li>Dunn指数（Dunn Index，简称DI）</li>
</ul>
<h2 id="（三）距离计算"><a href="#（三）距离计算" class="headerlink" title="（三）距离计算"></a>（三）距离计算</h2><p>给定样本xi=（xi1，xi2；…；xin），与xj=（xj1；xj2；…；xjn），最常用的是”闵可夫斯基距离“（Minkowski distance），<br><img src="http://upload-images.jianshu.io/upload_images/4905018-d08a5c6b691af278.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.18"><br>p=2时，闵可夫斯基距离即欧氏距离（Euclidean distance），<br><img src="http://upload-images.jianshu.io/upload_images/4905018-19f97beea58a9344.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.19"><br>p=1时，闵可夫斯基距离即曼哈顿距离（Manhattan distance），<br><img src="http://upload-images.jianshu.io/upload_images/4905018-5834c54b0862cbe8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.20"><br>上面的距离计算式都是事先定义好的，但在不少现实任务中，有必要基于数据样本来确定合适的距离计算式，这可通过”距离度量学习“（distance metric learning）来实现。</p>
<h2 id="（四）原型聚类"><a href="#（四）原型聚类" class="headerlink" title="（四）原型聚类"></a>（四）原型聚类</h2><p>原型聚类亦称”基于原型的聚类“（prototype-based clustering），此类算法假设聚类结构能通过一组原型刻画，在现实聚类任务中极为常用。通常情形下，算法先对原型进行初始化，然后对原型进行迭代更新求解。采用不同的原型表示、不同的求解方式，将产生不同的算法。</p>
<h3 id="1-k均值算法"><a href="#1-k均值算法" class="headerlink" title="1. k均值算法"></a>1. k均值算法</h3><p>给定样本集D={x1，x2，…，xm}，”k均值“（k-means）算法针对聚类所得簇划分C={C1，C2，…，Ck}最小化平方误差，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-4c4a5758b7580f82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9.24"><br>其中，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-f7a306463c42cd08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="公式"><br>x是簇Ci的均值向量。直观来看，上面式子在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，E值越小则簇内样本相似度越高。</p>
<h3 id="2-学习向量量化"><a href="#2-学习向量量化" class="headerlink" title="2. 学习向量量化"></a>2. 学习向量量化</h3><p>与k均值算法类似，“学习向量量化”（Learning Vector Quantization，简称LVQ）也是试图找到一组原型向量来刻画聚类结构，但与一般的聚类算法不同的是，LVQ假设数据样本带有类别标记，学习过程用样本的这些监督信息来辅助聚类。</p>
<h3 id="3-高斯混合聚类"><a href="#3-高斯混合聚类" class="headerlink" title="3. 高斯混合聚类"></a>3. 高斯混合聚类</h3><p>与k均值、LVQ用原型向量来刻画聚类结构不同，高斯混合（Mixture-of-Gaussian）聚类采用概率模型来表达聚类原型。</p>
<h2 id="（五）密度聚类"><a href="#（五）密度聚类" class="headerlink" title="（五）密度聚类"></a>（五）密度聚类</h2><p>密度聚类亦称“基于密度的聚类”（density-based clustering），此类算法假设聚类结构能通过样本分布的紧密程度确定。通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。<br>DBSCAN是一种著名的密度聚类算法。</p>
<h2 id="（六）层次聚类"><a href="#（六）层次聚类" class="headerlink" title="（六）层次聚类"></a>（六）层次聚类</h2><p>层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略。<br>AGNES是一种采用自底向上聚合策略的层次聚类算法。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/06/《机器学习》笔记-集成学习（8）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/06/《机器学习》笔记-集成学习（8）/" itemprop="url">
                  《机器学习》笔记-集成学习（8）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-06T09:06:08+08:00">
                2018-02-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>个体与集成</li>
<li>Boosting</li>
<li>Bagging与随机森林</li>
<li>集合策略</li>
<li>多样性</li>
</ul>
<h2 id="（一）个体与集成"><a href="#（一）个体与集成" class="headerlink" title="（一）个体与集成"></a>（一）个体与集成</h2><p>集成学习（ensemble learning）的一般结构：先产生一组“个体学习器”（individual learner），再用某种策略将他们结合起来，如下图所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-945d7aca6c56b4ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图8.1"><br>个体学习器通常由一个现有的学习算法从训练数据产生：</p>
<ul>
<li>只包含同种类型的个体学习器，这样的集成是“同质”的（homogeneous）。同质集成中的个体学习器亦称为”基学习器“（base learning），相应的学习算法称为”基学习算法“（base learning algorithm）。</li>
<li><p>集成也可包含不同类型的个体学习器，这样集成是”异质“的（heterogeneous）。相应的个体学习器，常称为”组件学习器“（component learning）或直接称为个体学习器。<br>在一般的经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比坏的好一些，比好的要坏一些。集成学习把多个学习器结合起来，如何能获得比最好的单一学习器更好的性能呢？<br>考虑一个简单的例子：在二分类任务中，假定三个分类器在三个测试样本的表现如下图所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-1df57820f98630cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图8.2"><br>其中，√表示分类正确，x表示分类错误，集成学习的结果通过投票法（voting）产生，即“少数服从多数”。这个简单的例子显示出：要获得好的集成，个体学习器应“好而不同”。个体学习器要有一定的“准确性”，即学习器不能太坏，而且要有“多样性”（diversity），即学习器之间有差异。事实上，如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心。<br>根据个体学习器的生成方式，目前集成学习的方法大致可分为两大类：</p>
</li>
<li><p>个体学习器间存在强依赖关系、必须串行生成的序列化方法，代表是Boosting；</p>
</li>
<li>个体学习器间不存在强依赖关系、可同时生成的并行化方法，代表是Baggig和“随机森林”（Random Forest）；</li>
</ul>
<h2 id="（二）Boosting"><a href="#（二）Boosting" class="headerlink" title="（二）Boosting"></a>（二）Boosting</h2><p>Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多的关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直到基学习器数目达到事先指定的值T，最终将这T个学习器进行加权结合。<br>Boosting族算法最著名的代表是AdaBoost。AdaBoost有多种推导方式，比较容易理解的是基于“加性模型”（additive model）即基学习器线性组合，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-8dbf74c811802eff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.4"><br>来最小化指数损失函数（exponential loss function），<br><img src="http://upload-images.jianshu.io/upload_images/4905018-c6d176a094ead7d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="8.5"></p>
<h2 id="（三）Bagging与随机森林"><a href="#（三）Bagging与随机森林" class="headerlink" title="（三）Bagging与随机森林"></a>（三）Bagging与随机森林</h2><p>欲得到泛化性能强的集成，集成中的个体学习器应尽可能独立。虽然“独立”在显示任务中无法做到，但可以设法使基学习器尽可能具有较大差异。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。然而，为获得更好的集成，我们还同时希望个体学习器不能太差。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足进行有效学习，这显然无法确保产生出比较好的基学习器。为考虑这个问题，我们可考虑使用相互有交叠的采样子集。</p>
<h3 id="1-Bagging"><a href="#1-Bagging" class="headerlink" title="1. Bagging"></a>1. Bagging</h3><p>Bagging是并行式集成学习方法最著名的代表，从名字即可看出，它直接基于前面介绍过的自助采样法（bootstrap sampling）。从偏差-方差分解角度看，Bagging主要关注降低方差。</p>
<h3 id="2-随机森林"><a href="#2-随机森林" class="headerlink" title="2. 随机森林"></a>2. 随机森林</h3><p>随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。<br>随机森林对Bagging只做了小改动，但是与Bagging中基学习器的“多样性”仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步提升。</p>
<h2 id="（四）组合策略"><a href="#（四）组合策略" class="headerlink" title="（四）组合策略"></a>（四）组合策略</h2><p>学期器结合可能从三个方面带来好处：</p>
<ul>
<li>从统计的方面看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器减小这一风险；</li>
<li>从计算的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险；</li>
<li>从表示的方面来看，某些但学习器则肯定无效，而通过结合多个学习器，由于响应的假设空间有所扩大，有可能学得更好的近似。<br>直观的示意图如下所示，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-c5c53a1182e7e3e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图8.8"><br>集成学习常见策略有：</li>
<li>平均法</li>
<li>投票法</li>
<li>学习法</li>
</ul>
<h2 id="（五）多样性"><a href="#（五）多样性" class="headerlink" title="（五）多样性"></a>（五）多样性</h2><h3 id="误差-分歧分解"><a href="#误差-分歧分解" class="headerlink" title="误差-分歧分解"></a>误差-分歧分解</h3><p>欲构建泛化能力强的集成，个体学习器应“好而不同”，其中，“误差-分歧分解”（error-ambiguity decomposition）是一个简单的理论分析方法。但该推到过程只适用于回归学习，难以直接推广到分类学习任务中。</p>
<h3 id="多样性度量"><a href="#多样性度量" class="headerlink" title="多样性度量"></a>多样性度量</h3><p>多样性度量（diversity measure）是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。常用的多样性度量包括：</p>
<ul>
<li>不合度量（disagreement measure）</li>
<li>相关系数（correlation coefficient）</li>
<li>Q-统计量（Q-statistics）</li>
<li>k-统计量（k-statistics）</li>
</ul>
<h3 id="多样性增强"><a href="#多样性增强" class="headerlink" title="多样性增强"></a>多样性增强</h3><p>在集成学习中需有效地生成多样性大的个体学习器。与简单地直接用初始数据训练出个体学习器相比，一般思路是在学习过程中引入随机性，常见的做法主要有，</p>
<ul>
<li>数据样本扰动</li>
<li>输入属性扰动</li>
<li>输出表示扰动</li>
<li>算法参数扰动</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/03/《机器学习》笔记-贝叶斯分类器（7）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/03/《机器学习》笔记-贝叶斯分类器（7）/" itemprop="url">
                  《机器学习》笔记-贝叶斯分类器（7）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-03T09:29:34+08:00">
                2018-02-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="写在最前面"><a href="#写在最前面" class="headerlink" title="写在最前面"></a>写在最前面</h1><p>如今机器学习和深度学习如此火热，相信很多像我一样的普通程序猿或者还在大学校园中的同学，一定也想参与其中。不管是出于好奇，还是自身充电，跟上潮流，我觉得都值得试一试。对于自己，经历了一段时间的系统学习（参考<a href="https://zhuanlan.zhihu.com/p/30980999" target="_blank" rel="external">《机器学习/深度学习入门资料汇总》</a>），现在计划重新阅读《机器学习》[周志华]和《深度学习》[Goodfellow et al]这两本书，并在阅读的过程中进行记录和总结。这两本是机器学习和深度学习的入门经典。笔记中除了会对书中核心及重点内容进行记录，同时，也会增加自己的理解，包括过程中的疑问，并尽量的和实际的工程应用和现实场景进行结合，使得知识不只是停留在理论层面，而是能够更好的指导实践。记录笔记，一方面，是对自己先前学习过程的总结和补充。 另一方面，相信这个系列学习过程的记录，也能为像我一样入门机器学习和深度学习同学作为学习参考。</p>
<h1 id="章节目录"><a href="#章节目录" class="headerlink" title="章节目录"></a>章节目录</h1><ul>
<li>贝叶斯决策论</li>
<li>极大似然估计</li>
<li>朴素贝叶斯分类器</li>
<li>半朴素贝叶斯分类器</li>
<li>贝叶斯网</li>
<li>EM算法</li>
</ul>
<h2 id="（一）贝叶斯决策论"><a href="#（一）贝叶斯决策论" class="headerlink" title="（一）贝叶斯决策论"></a>（一）贝叶斯决策论</h2><p>贝叶斯决策论（Bayesian decision theory）是概率框架下的基本方法。<br>假设有N种可能的类别标记，即y={c1，c2，…，cN}，λij是一个将真实标记为cj的样本误分类为ci产生的期望损失（expected loss），即在样本x上的“条件风险”（conditional rsik），<br><img src="http://upload-images.jianshu.io/upload_images/4905018-e079987c76ecc874.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.1"><br>我们的任务是寻找一个判断准则，以最小化总体风险。<br>欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P(c|x)。然而，在现实任务中这通常难以直接获得。从这个角度来看，机器学习所要实现的是基于有限的训练样本尽可能准确的估计出后验证概率P(c|x)。大体来说主要有两种策略：</p>
<ul>
<li>给定x，可通过直接建模P(c|x)来预测c，这样得到的是“判别式模型”（discriminative models）</li>
<li>先对联合概率分布P（x，c）建模，然后再由此获得P（c|x），这样得到的是“生成式模型”（generative models）。<br>显然，前面介绍的决策树、BP神经网络、支持向量机等，都可归入判别式模型的范畴。对于生成式模型来说，必然考虑，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-ee918bcf7cea6371.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.7"><br>基于贝叶斯定理可写成，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-8c2c457ff70256fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.8"><br>对给定样本x，证据因子P(x)与类标记无关。根据大数定理，先验概率P(c)可通过各类样本出现的频率来进行估计。因此，估计P(x|c)的问题就主要转换为如何基于训练样本D来估计似然P(x|c)。</li>
</ul>
<h2 id="（二）极大似然估计"><a href="#（二）极大似然估计" class="headerlink" title="（二）极大似然估计"></a>（二）极大似然估计</h2><p>估计类条件概率的一种常见策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。<br>概率模型的训练过程就是参数估计（parameter estimation）过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：</p>
<ul>
<li>频率主义学派（Frequentist）认为参数虽然未知，但确实客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值。</li>
<li>贝叶斯学派（Bayesian）则认为参数是未观察到的随机变量，其本身也有分布，因此，可假设参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。<br>书中的介绍来自频率主义学派的极大似然估计（Maximum Likelihood Estimation，简称MLE），这是根据数据采样来估计概率分布参数的经典方法。</li>
</ul>
<h2 id="（三）朴素贝叶斯分类器"><a href="#（三）朴素贝叶斯分类器" class="headerlink" title="（三）朴素贝叶斯分类器"></a>（三）朴素贝叶斯分类器</h2><p>基于贝叶斯公式来估计后验概率P(c|x)的主要困难在于，类条件概率P(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。为了避开这个障碍，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”（attribute conditional independence assumption）。即对已知类别，假设所有属性相互独立。换言之，假设每个属性独立地对分类结果发生影响。<br>基于属性条件独立性假设，条件概率P(c|x)可重写为，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-1286bdbbc17482f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.14"><br>其中d为属性数目，xi为x在第i个属性上的取值。<br>由于对所有类别来说P(x)相同，因此贝叶斯判定准则可写为，<br><img src="http://upload-images.jianshu.io/upload_images/4905018-6a7f3ba5c5f7ac3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7.15"><br>这就是朴素贝叶斯分类器的表达式。</p>
<h2 id="（四）半朴素贝叶斯分类器"><a href="#（四）半朴素贝叶斯分类器" class="headerlink" title="（四）半朴素贝叶斯分类器"></a>（四）半朴素贝叶斯分类器</h2><p>为了降低贝叶斯公式中估计后验概率P(c|x)的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，因此产生了一类称为“半朴素贝叶斯分类器”（semi-naive Bayes classifiers）的学习方法。</p>
<h2 id="（五）贝叶斯网"><a href="#（五）贝叶斯网" class="headerlink" title="（五）贝叶斯网"></a>（五）贝叶斯网</h2><p>贝叶斯网（Bayesian network）亦称“信念网”（belief network），它借助有向无环图（Directed Acyclic Graph，简称DAG）来刻画属性之间的依赖关系，并使用条件概率表（Conditional Probability Table，简称CPT）来描述属性的联合概率分布。</p>
<h2 id="（六）EM算法"><a href="#（六）EM算法" class="headerlink" title="（六）EM算法"></a>（六）EM算法</h2><p>在前面的讨论中，我们一直假设训练样本所有属性变量的值都已被观测到，即训练样本是“完整”的。但现实应用中往往遇到“不完整”的训练样本。在存在“未观测”变量的情况下，是否仍能对模型参数进行估计呢？<br>EM（Expectation-Maximization）算法就是常用的估计参数隐变量的利器。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Liu Caiquan" />
          <p class="site-author-name" itemprop="name">Liu Caiquan</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">68</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liu Caiquan</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

# 此位置插入以下代码
<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
</div>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  

  

  

</body>
</html>
