<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="从零开始学习自然语言处理(NLP)," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="1 BERT模型介绍BERT是NLP任务的集大成者。 发布时，在GLUE 上的效果排名第一。在语义表征方面将基于浅层语义表征的词向量，加强为深层语义特征向量。同时，引入上下文特征，解决了词向量一词多义的问题。在知识迁移方面能够在海量的无监督文本训练语料中，有效的抽取语义特征。Pre-Train+Fine-Turning的模式下，非常实用。在计算效率方面BERT基于Transformer实现，采用s">
<meta name="keywords" content="从零开始学习自然语言处理(NLP)">
<meta property="og:type" content="article">
<meta property="og:title" content="《从零开始学习自然语言处理(NLP)》-BERT模型推理加速总结（5）">
<meta property="og:url" content="http://yoursite.com/2019/11/01/《从零开始学习自然语言处理-NLP-》-BERT模型推理加速总结（5）/index.html">
<meta property="og:site_name" content="才权的博客">
<meta property="og:description" content="1 BERT模型介绍BERT是NLP任务的集大成者。 发布时，在GLUE 上的效果排名第一。在语义表征方面将基于浅层语义表征的词向量，加强为深层语义特征向量。同时，引入上下文特征，解决了词向量一词多义的问题。在知识迁移方面能够在海量的无监督文本训练语料中，有效的抽取语义特征。Pre-Train+Fine-Turning的模式下，非常实用。在计算效率方面BERT基于Transformer实现，采用s">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-b49abd72fe4e5aa04c8f102bcbfc1d10_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-0b13b8f55e34ef76907cf51a6d8f8822_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-9ba0e98a1ecd826554bb1b0cd89190f0_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-67b1d044bd7305133bf32f28dc30217a_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-fd3a4a3a757235bd81db0d0f92223cb6_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-3fb25dba91373640286cd746d2cc0d07_hd.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-069c19888f2154b37f801469bdaa8a9e_hd.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-7e539f13af0ec75625a7111704a2a091_hd.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-bd4fd2a6273989bce2094bd4c6560760_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-382269ae01b54d4270dc84c3462c5bdb_hd.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-44bf23e1ad85a509c3dc585103aee94b_hd.jpg">
<meta property="og:updated_time" content="2019-11-01T13:09:11.793Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《从零开始学习自然语言处理(NLP)》-BERT模型推理加速总结（5）">
<meta name="twitter:description" content="1 BERT模型介绍BERT是NLP任务的集大成者。 发布时，在GLUE 上的效果排名第一。在语义表征方面将基于浅层语义表征的词向量，加强为深层语义特征向量。同时，引入上下文特征，解决了词向量一词多义的问题。在知识迁移方面能够在海量的无监督文本训练语料中，有效的抽取语义特征。Pre-Train+Fine-Turning的模式下，非常实用。在计算效率方面BERT基于Transformer实现，采用s">
<meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-b49abd72fe4e5aa04c8f102bcbfc1d10_hd.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/11/01/《从零开始学习自然语言处理-NLP-》-BERT模型推理加速总结（5）/"/>





  <title> 《从零开始学习自然语言处理(NLP)》-BERT模型推理加速总结（5） | 才权的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4e94c4c1ded5d320541370545af638ea";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">才权的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/01/《从零开始学习自然语言处理-NLP-》-BERT模型推理加速总结（5）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liu Caiquan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="才权的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                《从零开始学习自然语言处理(NLP)》-BERT模型推理加速总结（5）
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-01T21:06:23+08:00">
                2019-11-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-BERT模型介绍"><a href="#1-BERT模型介绍" class="headerlink" title="1 BERT模型介绍"></a>1 BERT模型介绍</h2><p>BERT是NLP任务的集大成者。 发布时，在GLUE 上的效果排名第一。<br><strong>在语义表征方面</strong><br>将基于浅层语义表征的词向量，加强为深层语义特征向量。同时，引入上下文特征，解决了词向量一词多义的问题。<br><strong>在知识迁移方面</strong><br>能够在海量的无监督文本训练语料中，有效的抽取语义特征。Pre-Train+Fine-Turning的模式下，非常实用。<br><strong>在计算效率方面</strong><br>BERT基于Transformer实现，采用self-attention机制，采用并行的矩阵运算，替代RNN的串行执行，有效提升模型计算效率。<br><strong>在任务设计方面</strong><br>BERT提供了分类、相似度计算（也是一种分类）、序列标注等NLP任务模板，基本覆盖了NLP任务的大多数模型应用场景。<br>具体的BERT介绍可以参考Google的论文：《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》<br>在学习BERT之前，建议先熟悉Transformer，具体可以参考Google论文：《Attention Is All You Need》<br>直接看论文可能不好理解，建议先参考网上的Blog：<br>图解Transformer：《The Illustrated Transformer》<br>图解BERT：《The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)》<br>很多国内的博客都是上面两篇博客的翻译或者引用。<br>BERT在NLP预训练领域确实有里程碑的意义，这篇博客做了不错的总结：《从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史》</p>
<h2 id="2-BERT模型的困境"><a href="#2-BERT模型的困境" class="headerlink" title="2 BERT模型的困境"></a>2 BERT模型的困境</h2><p>虽然BERT有各种优点，在项目落地上却存在缺陷：计算复杂度过高。通过BERT模型的参数量，感受下：<br><img src="https://pic1.zhimg.com/80/v2-b49abd72fe4e5aa04c8f102bcbfc1d10_hd.jpg" alt="BERT参数量"><br>数据来源：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT</a><br>针对BERT模型，我们从预训练（Pre-train）、Fine-tuning、推理三个维度来看模型的计算耗时。<br><strong>模型训练（Pre-train）时间：</strong><br><img src="https://pic3.zhimg.com/80/v2-0b13b8f55e34ef76907cf51a6d8f8822_hd.jpg" alt="Pre-train时间"><br>数据来源：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT</a><br><strong>Fine-tuning时间：</strong><br><img src="https://pic1.zhimg.com/80/v2-9ba0e98a1ecd826554bb1b0cd89190f0_hd.jpg" alt="Fine-tuning时间"><br>数据来源：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT</a><br><strong>模型推理时间：</strong><br><img src="https://pic3.zhimg.com/80/v2-67b1d044bd7305133bf32f28dc30217a_hd.jpg" alt="模型推理时间"><br><img src="https://pic3.zhimg.com/80/v2-fd3a4a3a757235bd81db0d0f92223cb6_hd.jpg" alt="模型推理时间"><br>数据来源：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT</a><br>从上面的数据来看，预训练（Pre-train）要求的计算资源最大，耗时最长。自己做预训练相对较难。还好Google提供预训练模型（包括中文的预训练模型），git地址：<a href="https://github.com/google-research/bert" target="_blank" rel="external">https://github.com/google-research/bert</a><br>如果，计算资源充裕，可以使用自己的语料进行预训练，从实际的项目经验反馈来看，用自己的业务相关语料进行预训练，对模型的最终效果，相对于使用Google通用的预训练模型，有小幅提升。<br>Fine-tuning的计算在实际项目应用中基本可以接受，这里不做特别的深入。<br>最后需要关注的就是模型推理，而这也是服务上线最需要关注的。后面我们的分析重点，也集中在如何提升BERT模型的推理效率。</p>
<h2 id="3-BERT模型推理加速方案汇总"><a href="#3-BERT模型推理加速方案汇总" class="headerlink" title="3 BERT模型推理加速方案汇总"></a>3 BERT模型推理加速方案汇总</h2><p>BERT的模型优化方案，大概有下面几个方面：</p>
<h3 id="3-1-编译优化"><a href="#3-1-编译优化" class="headerlink" title="3.1 编译优化"></a>3.1 编译优化</h3><p>这里的编译优化主要指计算框架自身的编译优化，以TensorFlow的XLA为例，<br><img src="https://pic4.zhimg.com/80/v2-3fb25dba91373640286cd746d2cc0d07_hd.jpg" alt="xla编译优化"><br>数据来源：<a href="https://www.tensorflow.org/xla" target="_blank" rel="external">https://www.tensorflow.org/xla</a><br>从实际的数据来看，TensorFlow的XLA优化能力有限，&gt;1.15倍。</p>
<h3 id="3-2-知识蒸馏"><a href="#3-2-知识蒸馏" class="headerlink" title="3.2 知识蒸馏"></a>3.2 知识蒸馏</h3><p>知识蒸馏是一个不错的模型压缩方案，通过小模型（student）来学习大模型（teacher）的能力。但一般来说，通过蒸馏获得小模型，相对于原来的小模型有算法效果提升，但不能保证能完全达到大模型（teacher）的效果。<br>TinyBERT就是使用了这种思路，具体可以参考paper《TinyBERT: Distilling BERT for Natural Language Understanding》</p>
<h3 id="3-3-混合精度（Nvidia-TensorCore）"><a href="#3-3-混合精度（Nvidia-TensorCore）" class="headerlink" title="3.3 混合精度（Nvidia-TensorCore）"></a>3.3 混合精度（Nvidia-TensorCore）</h3><p>一般模型的参数使用FP32（单精度）进行保存和计算。特定的计算平台，比如英伟达的P100，可以使用单个的FP32寄存器，缓存两个FP16（半精度）变量，并进行并行计算，参考《Nvidia GPU的浮点计算能力》。</p>
<p>同时，英伟达的瓦特架构（Tesla V100）和图灵架构（Tesla T4）有TensorCore计算单元，可以同时完成混合精度矩阵的乘加运算。具体可以参考《Video Series: Mixed-Precision Training Techniques Using Tensor Cores for Deep Learning》。</p>
<p>将模型中的FP32变量替换成FP16，可以有效的压缩模型大小，同时，可以提升模型的训练及推理速度。但对模型效果会不会有影响呢？答案，肯定是不会的。具体原因可以参考这篇paper《Mixed Precision Training》。<br>文章主要提了三个方法：<br><strong>1 FP32 MASTER COPY OF WEIGHTS</strong><br><img src="https://pic3.zhimg.com/80/v2-069c19888f2154b37f801469bdaa8a9e_hd.jpg" alt="FP32 MASTER COPY OF WEIGHTS"><br>简单的说就是把FP32的变量先拷贝一份，然后把所有的变量都量化成FP16。在前向和反向计算时都使用FP16，只是在最后更新参数时转化为FP32。<br>这样做也很好理解，更新参数时，一方面，会进行整个batch参数相加（然后取均值），转化成FP32避免向上溢出；另一方面，非常小的梯度会乘以学习率（一般&lt;1），转化成FP32避免向下溢出。<br><strong>2 LOSS SCALING</strong><br>就是计算梯度时，梯度一般都会非常小，这里将梯度先乘以一个较大的系数进行放大，避免反向传播发生下溢出，最终再除以一个相同系数进行缩小，恢复正常值，从而在FP16精度的情况下，完成计算。<br><strong>3 ARITHMETIC PRECISION</strong><br>这里其实就是介绍了下英伟达的专用硬件模块TensorCore，可以同时完成混合精度的矩阵乘加运算。<br>ps：这篇paper的作者来自百度和英伟达。<br>在Nvidia平台上，使用混合精度也非常方便，TensorFlow直接集成了Nvidia的混合精度特性，具体操作如下：<br><img src="https://pic2.zhimg.com/80/v2-7e539f13af0ec75625a7111704a2a091_hd.jpg" alt="混精操作"><br>参考《Automatic Mixed Precision for Deep Learning》</p>
<h3 id="3-4-Nvidia-编译优化（TensorRT）"><a href="#3-4-Nvidia-编译优化（TensorRT）" class="headerlink" title="3.4 Nvidia-编译优化（TensorRT）"></a>3.4 Nvidia-编译优化（TensorRT）</h3><p>除了TensorFlow自己的XLA优化编译之外，Nvidia推出了自己的模型编译优化方案TensorRT。XLA完成的是通用平台的模型压缩和剪枝优化编译，TensorRT更侧重跟自家平台的结合。<br>在Nvidia的GPU平台上TensorRT应该比XLA有更好的优化效果。TensorRT其实是一个较大的工具体系，混合精度、模型优化编译、自定义Plugin、TensorRT Serving都是它的组成部分。所以，不要单纯的理解TensorRT只是用来做编译优化的。</p>
<h3 id="3-5-Nvidia-Batch合并（TensorRT-Inference-Serving）"><a href="#3-5-Nvidia-Batch合并（TensorRT-Inference-Serving）" class="headerlink" title="3.5 Nvidia-Batch合并（TensorRT Inference Serving）"></a>3.5 Nvidia-Batch合并（TensorRT Inference Serving）</h3><p>Batch合并操作充分利用了计算平台并行计算的能力。假设有8个请求，一种方式是每个请求顺序发给模型，然后，串行获取结果；另一种是8个请求合并成一个batch请求模型，然后，并行获取结果。在GPU这样的并行计算平台，后者的计算效率更高。<br>TensorRT Inference Serving就是利用这个特点，在可容忍的时间相应时间内，尽量合并请求，进行batch处理，从而，提升服务的吞吐量。这是BERT模型在TensorRT Inference Serving的表现：<br><img src="https://pic1.zhimg.com/80/v2-bd4fd2a6273989bce2094bd4c6560760_hd.jpg" alt="Inference Serving的表现"><br>数据来源：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT</a><br>使用TensorRT Inference Serving后，服务的吞吐量有明显的提升。</p>
<h3 id="3-6-Nvidia-自定义Plugin（TensorRT）"><a href="#3-6-Nvidia-自定义Plugin（TensorRT）" class="headerlink" title="3.6 Nvidia-自定义Plugin（TensorRT）"></a>3.6 Nvidia-自定义Plugin（TensorRT）</h3><p>Plugin的意思是指模型中的特定计算操作。可以是重写TensorFlow中的算子（如softmax操作），也可以是自定义的矩阵乘加运算。TensorRT提供了C++和Python层的接口，可以进行自定义Plugin的编写。<br>这些自定义的操作，一方面，可以充分利用Nvidia GPU的计算特性；另一方面，在网络编译优化阶段也会有更好的效果。TensorRT的具体使用可以参考《TensorRT Developer Guide》。<br>这部分会是BERT性能优化最大的地方，后面的内容会围绕这一点重点展开。</p>
<h2 id="4-BERT模型推理加速优化实践"><a href="#4-BERT模型推理加速优化实践" class="headerlink" title="4 BERT模型推理加速优化实践"></a>4 BERT模型推理加速优化实践</h2><p>Nvidia对BERT加速做了专门的工作，也获得不不错的效果，先看结果感受下：<br><img src="https://pic4.zhimg.com/80/v2-382269ae01b54d4270dc84c3462c5bdb_hd.jpg" alt="BERT模型推理加速优化实践"><br>数据来源：<a href="https://devblogs.nvidia.com/nlu-with-tensorrt-bert/" target="_blank" rel="external">https://devblogs.nvidia.com/nlu-with-tensorrt-bert/</a></p>
<h3 id="4-1-基于TensorRT-API实现"><a href="#4-1-基于TensorRT-API实现" class="headerlink" title="4.1 基于TensorRT API实现"></a>4.1 基于TensorRT API实现</h3><p>在阅读理解任务上的响应时间压缩到了2.2ms，可见优化效果是非常明显的。英伟达开源了整个实验的数据和代码，具体可以参考《Real-Time Natural Language Understanding with BERT Using TensorRT》<br>基本思路是：<br>1 Fine-tuning获得的TensorFlow的ckpt模型（如果需要开启混合精度能力，需要在Fine-tuning步骤完成，建议直接使用英伟达修改过的BERT模型，参考《DeepLearningExamples/TensorFlow/LanguageModeling/BERT》）<br>2 使用TensorRT重新实现BERT，主要是Transformer部分（可以理解自定义了很多Plugin），同时，也包含任务相关的部分代码（如分类任务和阅读理解任务的实现是不同的）<br>3 使用TensorRT接口加载ckpt模型文件并获取参数<br>4 将参数输入给重新实现的BERT网络<br>5 优化编译并输出优化后的模型文件（TensorRT-engine文件）<br>6 加载TensorRT-engine文件，并进行推理<br>文中的示例只提供了阅读理解的实现，如果是分类问题，需要自己使用TensorRT API编写Transformer输出后，任务相关的代码，并对其参数。这里需要熟悉TensorRT的API，操作起来比较麻烦。</p>
<h3 id="4-2-基于Fast-Transformer实现"><a href="#4-2-基于Fast-Transformer实现" class="headerlink" title="4.2 基于Fast Transformer实现"></a>4.2 基于Fast Transformer实现</h3><p>如果先看了《Real-Time Natural Language Understanding with BERT Using TensorRT》这篇文章，并尝试文中的方法进行分类分类任务。再看到基于Fast Transformer的方案，肯定觉得自己走了弯路（PS，我就是这样，汗~）。<br>BERT的核心是Transformer，Nvidia很贴心的把Transformer（主要是Encoder）以组件的形式进行了加速优化，并作为单独的开源项目发布（实现原理和上面一致）。项目参考《Faster Transformer》<br>简单来说，就是编写推理服务时，还是继续使用TensorFlow，只是其中的Google默认实现的Transformer被Nvidia换成了自己的定制实现模块。Transformer之外的部分原封不动。这样就既实现了BERT的计算加速，又保证了使用的灵活性。效果还是蛮不错的：<br><img src="https://pic4.zhimg.com/80/v2-44bf23e1ad85a509c3dc585103aee94b_hd.jpg" alt="fast-transformer优化效果"><br>数据来源：<a href="https://github.com/NVIDIA/DeepLearningExamples/blob/master/FasterTransformer/sample/tensorflow_bert/sample.md" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/blob/master/FasterTransformer/sample/tensorflow_bert/sample.md</a><br>这里多说一句，后面Nvidia会开源Fast Transformer 2.0，核心就是实现了Transformer的Decoder。<br>另外，TensorRT和Fast Transformer Nvidia有专门的在线研讨会（虽然过期了，但可以回看），这里推荐两个，《利用 TensorRT 自由搭建高性能推理模型》，《Faster Transformer 介绍》</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了BERT的基本情况，面临的上线问题，以及加速优化的各种方案及原理。最后提出了基于Fast Transformer的通用加速方案。</p>
<h2 id="资料参考"><a href="#资料参考" class="headerlink" title="资料参考"></a>资料参考</h2><p>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》：<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="external">https://arxiv.org/abs/1810.04805</a></p>
<p>《Attention Is All You Need》：<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">https://arxiv.org/abs/1706.03762</a></p>
<p>《The Illustrated Transformer》：<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="external">https://jalammar.github.io/illustrated-transformer/</a></p>
<p>《The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)》：<a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="external">https://jalammar.github.io/illustrated-bert/</a></p>
<p>《从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史》：<a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/49271699</a></p>
<p>《TinyBERT: Distilling BERT for Natural Language Understanding》：<a href="https://arxiv.org/abs/1909.10351" target="_blank" rel="external">https://arxiv.org/abs/1909.10351</a></p>
<p>《Nvidia GPU的浮点计算能力》：<a href="https://weibo.com/ttarticle/p/show?id=2309403987017473113077" target="_blank" rel="external">https://weibo.com/ttarticle/p/show?id=2309403987017473113077</a></p>
<p>《Video Series: Mixed-Precision Training Techniques Using Tensor Cores for Deep Learning》：<a href="https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/?ncid=so-twi-dplgdrd3-73821" target="_blank" rel="external">https://devblogs.nvidia.com/video-mixed-precision-techniques-tensor-cores-deep-learning/?ncid=so-twi-dplgdrd3-73821</a></p>
<p>《Mixed Precision Training》：<a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="external">https://arxiv.org/abs/1710.03740</a></p>
<p>《Automatic Mixed Precision for Deep Learning》：<a href="https://developer.nvidia.com/automatic-mixed-precision" target="_blank" rel="external">https://developer.nvidia.com/automatic-mixed-precision</a></p>
<p>《TensorRT Developer Guide》：<a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#extending" target="_blank" rel="external">https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#extending</a></p>
<p>《NVIDIA/tensorrt-inference-server》：<a href="https://github.com/NVIDIA/tensorrt-inference-server" target="_blank" rel="external">https://github.com/NVIDIA/tensorrt-inference-server</a></p>
<p>《Real-Time Natural Language Understanding with BERT Using TensorRT》：<a href="https://devblogs.nvidia.com/nlu-with-tensorrt-bert/" target="_blank" rel="external">https://devblogs.nvidia.com/nlu-with-tensorrt-bert/</a></p>
<p>《Faster Transformer》：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer</a></p>
<p>《利用 TensorRT 自由搭建高性能推理模型》：<a href="https://info.nvidia.com/223020-ondemand.html" target="_blank" rel="external">https://info.nvidia.com/223020-ondemand.html</a></p>
<p>《Faster Transformer 介绍》：<a href="https://mudu.tv/watch/3737641?key=502dfd75f66e66a8d5e4621c0887f617&amp;expire=600" target="_blank" rel="external">https://mudu.tv/watch/3737641?key=502dfd75f66e66a8d5e4621c0887f617&amp;expire=600</a></p>
<p>《DeepLearningExamples/TensorFlow/LanguageModeling/BERT》：<a href="https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT" target="_blank" rel="external">https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/LanguageModeling/BERT</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/从零开始学习自然语言处理-NLP/" rel="tag"># 从零开始学习自然语言处理(NLP)</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/13/混合精度对模型训练和推理的影响/" rel="next" title="混合精度对模型训练和推理的影响">
                <i class="fa fa-chevron-left"></i> 混合精度对模型训练和推理的影响
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/09/《从零开始学习自然语言处理-NLP-》-BERT推理加速实践（6）/" rel="prev" title="《从零开始学习自然语言处理(NLP)》-BERT推理加速实践（6）">
                《从零开始学习自然语言处理(NLP)》-BERT推理加速实践（6） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Liu Caiquan" />
          <p class="site-author-name" itemprop="name">Liu Caiquan</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">71</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-BERT模型介绍"><span class="nav-number">1.</span> <span class="nav-text">1 BERT模型介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-BERT模型的困境"><span class="nav-number">2.</span> <span class="nav-text">2 BERT模型的困境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-BERT模型推理加速方案汇总"><span class="nav-number">3.</span> <span class="nav-text">3 BERT模型推理加速方案汇总</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-编译优化"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 编译优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-知识蒸馏"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 知识蒸馏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-混合精度（Nvidia-TensorCore）"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 混合精度（Nvidia-TensorCore）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Nvidia-编译优化（TensorRT）"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 Nvidia-编译优化（TensorRT）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Nvidia-Batch合并（TensorRT-Inference-Serving）"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 Nvidia-Batch合并（TensorRT Inference Serving）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-Nvidia-自定义Plugin（TensorRT）"><span class="nav-number">3.6.</span> <span class="nav-text">3.6 Nvidia-自定义Plugin（TensorRT）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-BERT模型推理加速优化实践"><span class="nav-number">4.</span> <span class="nav-text">4 BERT模型推理加速优化实践</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-基于TensorRT-API实现"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 基于TensorRT API实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-基于Fast-Transformer实现"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 基于Fast Transformer实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">5.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#资料参考"><span class="nav-number">6.</span> <span class="nav-text">资料参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liu Caiquan</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
</div>



        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  

  

  

</body>
</html>
